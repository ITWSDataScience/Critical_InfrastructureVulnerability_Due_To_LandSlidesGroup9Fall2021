{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "# Reading and Storing Unique Lat, Lon Pairs\n",
    "df = pd.concat([pd.read_csv(\"../data/lat_lon_1.csv\"), pd.read_csv(\"../data/lat_lon_2.csv\")])[[\"latitude\", \"longitude\"]]\n",
    "store = pd.HDFStore(\"../data/data.h5\")\n",
    "store.put(\"latlon\", df, format=\"table\", append=True, data_columns=True)\n",
    "store.close()\n",
    "del df, store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File ../data/landslide.h5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-a8c374616869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Moving existing landslide data into the same store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlandslide_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/landslide.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m36000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/data_small.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/etc/anaconda3/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             raise compat.FileNotFoundError(\n\u001b[0;32m--> 371\u001b[0;31m                 'File %s does not exist' % path_or_buf)\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File ../data/landslide.h5 does not exist"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "# Moving existing landslide data into the same store\n",
    "\n",
    "landslide_store = pd.read_hdf(\"../data/landslide.h5\", iterator=True, chunksize=1000000, start=36000000)\n",
    "store = pd.HDFStore(\"../data/data_small.h5\")\n",
    "\n",
    "for chunk in landslide_store:\n",
    "    chunk[\"time\"] = chunk[\"date\"]\n",
    "    del chunk[\"date\"]\n",
    "    chunk = chunk[chunk['time'].dt.year == 2019]\n",
    "    print(len(chunk))\n",
    "    chunk.set_index([\"latitude\", \"longitude\", \"time\"], inplace=True)\n",
    "    if len(chunk):\n",
    "        store.put(\"landslide\", chunk, format=\"table\", append=True, data_columns=True)\n",
    "\n",
    "store.close()\n",
    "landslide_store.close()\n",
    "del store, landslide_store, chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "def dist(x, k):\n",
    "    return math.sqrt((x[\"latitude\"] - k[\"latitude\"])**2 + (x[\"longitude\"] - k[\"longitude\"])**2)\n",
    "\n",
    "def get_match(k, group):\n",
    "    print(\"started\")\n",
    "    start = time.time()\n",
    "    index = group.apply(lambda x: dist(x,k), axis=1).idxmin()\n",
    "    print(time.time() - start)\n",
    "    return index\n",
    "\n",
    "#reading, processing, and storing the earthquake data \n",
    "\n",
    "def energy_from_magnitude(m):\n",
    "    #https://www.usgs.gov/natural-hazards/earthquake-hazards/science/earthquake-magnitude-energy-release-and-shaking-intensity?qt-science_center_objects=0#qt-science_center_objects\n",
    "    return math.pow(10,(5.24 + (1.44 * m)))\n",
    "\n",
    "coord = pd.concat([pd.read_csv(\"../data/lat_lon_1.csv\"), pd.read_csv(\"../data/lat_lon_2.csv\")])[[\"latitude\", \"longitude\"]]\n",
    "df = pd.concat([pd.read_csv(\"../data/earthquake_northeast.csv\"), pd.read_csv(\"../data/earthquake_northwest.csv\")])[[\"time\", \"latitude\", \"longitude\", \"mag\"]]\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "df[\"energy\"] = df[\"mag\"].apply(energy_from_magnitude)\n",
    "df = df[df['time'].dt.year == 2019]\n",
    "\n",
    "df[\"square\"] = df.apply(lambda x: math.sqrt((x[\"latitude\"])**2 + (x[\"longitude\"])**2), axis=1)\n",
    "coord[\"square\"] = coord.apply(lambda x: math.sqrt((x[\"latitude\"])**2 + (x[\"longitude\"])**2), axis=1)\n",
    "df.sort_values(by=\"square\", inplace=True)\n",
    "coord.sort_values(by=\"square\", inplace=True)\n",
    "\n",
    "df= pd.merge_asof(df, coord, on=\"square\", direction=\"nearest\")\n",
    "df[\"latitude\"] = df[\"latitude_y\"]\n",
    "df[\"longitude\"] = df[\"longitude_y\"]\n",
    "\n",
    "\n",
    "del df[\"latitude_x\"], df[\"longitude_x\"], df[\"latitude_y\"], df[\"longitude_y\"], df[\"square\"]\n",
    "\n",
    "# df.apply(lambda x: math.sqrt((x[\"latitude\"] - k[\"latitude\"])**2 + (x[\"longitude\"] - k[\"longitude\"])**2), axis=1)\n",
    "# df[\"coord\"] = df.apply(lambda x: get_match(x, coord), axis=1)\n",
    "# df[\"latitude\"]  = df.apply(lambda x: coord[\"latitude\"][x[\"coord\"]], axis=1)\n",
    "# df[\"longitude\"]  = df.apply(lambda x: coord[\"longitude\"][x[\"coord\"]], axis=1)\n",
    "# del df[\"coord\"]\n",
    "df.set_index([\"latitude\", \"longitude\", \"time\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "store = pd.HDFStore(\"../data/data_small_3.h5\")\n",
    "store.put(\"earthquake\", df, format=\"table\", append=True, data_columns=True)\n",
    "store.close()\n",
    "\n",
    "# del df, store, coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "\n",
    "# Reading, Processing, and Storing the Precipiation Data\n",
    "df = pd.concat([pd.read_csv(\"../data/NorthEast_precipitation_data.csv\"), pd.read_csv(\"../data/NorthWest_precipitation_data.csv\")])\n",
    "df[\"Time\"] = pd.to_datetime(df[\"Date Time in YYYY-MM-DD\"])\n",
    "del df[\"Date Time in YYYY-MM-DD\"], df[df.columns[0]]\n",
    "df.columns = [\"latitude\", \"longitude\", \"precipitation\", \"time\"]\n",
    "df = df[df['time'].dt.year == 2019]\n",
    "coord = pd.concat([pd.read_csv(\"../data/lat_lon_1.csv\"), pd.read_csv(\"../data/lat_lon_2.csv\")])[[\"latitude\", \"longitude\"]]\n",
    "\n",
    "df[\"square\"] = df.apply(lambda x: math.sqrt((x[\"latitude\"])**2 + (x[\"longitude\"])**2), axis=1)\n",
    "coord[\"square\"] = coord.apply(lambda x: math.sqrt((x[\"latitude\"])**2 + (x[\"longitude\"])**2), axis=1)\n",
    "df.sort_values(by=\"square\", inplace=True)\n",
    "coord.sort_values(by=\"square\", inplace=True)\n",
    "\n",
    "df = pd.merge_asof(df, coord, on=\"square\", direction=\"nearest\")\n",
    "df[\"latitude\"] = df[\"latitude_y\"]\n",
    "df[\"longitude\"] = df[\"longitude_y\"]\n",
    "del df[\"latitude_x\"], df[\"longitude_x\"], df[\"latitude_y\"], df[\"longitude_y\"], df[\"square\"]\n",
    "\n",
    "# latlon = pd.read_hdf(\"../data/data.h5\", key=\"latlon\")\n",
    "# df[\"latitude\"] = df[\"latitude\"].map(lambda x: latlon[\"latitude\"][abs(latlon[\"latitude\"] - x).idxmin()])\n",
    "# df[\"longitude\"] = df[\"longitude\"].map(lambda x: latlon[\"longitude\"][abs(latlon[\"longitude\"] - x).idxmin()])\n",
    "# df[\"coord\"] = df.apply(lambda x: get_match(x, coord), axis=1)\n",
    "# df[\"latitude\"]  = df.apply(lambda x: coord[\"latitude\"][x[\"coord\"]], axis=1)\n",
    "# df[\"longitude\"]  = df.apply(lambda x: coord[\"longitude\"][x[\"coord\"]], axis=1)\n",
    "# del df[\"coord\"]\n",
    "\n",
    "\n",
    "df.set_index([\"latitude\", \"longitude\", \"time\"], inplace=True)\n",
    "store = pd.HDFStore(\"../data/data_small_3.h5\")\n",
    "store.put(\"precipitation\", df, format=\"table\", append=True, data_columns=True)\n",
    "store.close()\n",
    "# del df, store#,latlon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 landslide_prob\n",
      "latitude  longitude  time                      \n",
      "39.730192 -71.161238 2019-01-01        0.171767\n",
      "                     2019-04-01        0.185748\n",
      "                     2019-08-01        0.131431\n",
      "                     2019-10-01        0.197255\n",
      "                     2019-12-01        0.114612\n",
      "    latitude  longitude       time   mag        energy\n",
      "0  41.071858 -71.227904 2019-12-01  2.10  1.836538e+08\n",
      "1  41.363525 -71.344571 2019-12-01  1.80  6.792036e+07\n",
      "2  39.796858 -72.319571 2019-04-01  3.04  4.145720e+09\n",
      "3  39.905192 -72.277904 2019-02-01  1.40  1.803018e+07\n",
      "4  40.763525 -71.811238 2019-10-01  1.30  1.294196e+07\n",
      "                                 precipitation\n",
      "latitude  longitude  time                     \n",
      "39.730192 -71.161238 2019-01-01       0.171767\n",
      "                     2019-04-01       0.185748\n",
      "                     2019-08-01       0.131431\n",
      "                     2019-10-01       0.197255\n",
      "                     2019-12-01       0.114612\n",
      "4032\n"
     ]
    }
   ],
   "source": [
    "# Merging Land and Precipiation\n",
    "\n",
    "# Reading earthquake data into store\n",
    "def magnitude_from_energy(e):\n",
    "    return (math.log10(e) - 5.24) / 1.44\n",
    "\n",
    "def lat_lon_dist(lat1, lon1, lat2, lon2):\n",
    "    #https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n",
    "    R = 6363.0\n",
    "    \n",
    "    lat1 = math.radians(lat1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lon2 = math.radians(lon2)\n",
    "    dlon = abs(lon2 - lon1)\n",
    "    dlat = abs(lat2 - lat1)\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def felt_magnitude(lat1, lon1, lat2, lon2, energy):\n",
    "    divisor = math.pi * 2 * lat_lon_dist(lat1, lon1, lat2, lon2)\n",
    "    return energy / divisor\n",
    "\n",
    "precipitation = pd.read_hdf(\"../data/data_small_3.h5\", key=\"precipitation\")\n",
    "earthquake = pd.read_hdf(\"../data/data_small_3.h5\", key=\"earthquake\")\n",
    "earthquake = earthquake.reset_index()\n",
    "earthquake[\"time\"] = pd.to_datetime(earthquake[\"time\"].map(lambda x: x.year * 100 + x.month), format=\"%Y%m\")\n",
    "chunk = pd.read_hdf(\"../data/data_small_3.h5\", key=\"landslide\")#, iterator=True, chunksize=1000000)\n",
    "store = pd.HDFStore(\"../data/merged.h5\")\n",
    "# for chunk in landslides:\n",
    "chunk = chunk.reset_index()\n",
    "chunk.columns = [\"latitude\", \"longitude\", \"time\", \"landslide_prob\"]\n",
    "chunk.set_index([\"latitude\", \"longitude\", \"time\"], inplace=True)\n",
    "print(chunk.head())\n",
    "print(earthquake.head())\n",
    "print(precipitation.head())\n",
    "chunk = chunk.merge(earthquake, on=[\"latitude\", \"longitude\"])\n",
    "chunk = chunk.merge(precipitation, on=[\"latitude\", \"longitude\"])\n",
    "print(len(chunk))\n",
    "#     store.put(\"landslide\", chunk, format=\"table\", append=True, data_columns=True)\n",
    "del landslides\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Earthquake Data\n",
    "\n",
    "earthquake = pd.read_hdf(\"../data/data.h5\", key=\"earthquake\")\n",
    "earthquake = earthquake.reset_index()\n",
    "earthquake[\"time\"] = pd.to_datetime(earthquake[\"time\"].map(lambda x: x.year * 100 + x.month), format=\"%Y%m\")\n",
    "landslides = pd.read_hdf(\"../data/merged.h5\")\n",
    "l_groups = landslides.groupby(\"time\")\n",
    "e_groups = earthquake.groupby(\"time\")\n",
    "store = pd.HDFStore(\"../data/merged.h5\")\n",
    "\n",
    "for name, group in l_groups:\n",
    "    temp = e_groups.get_group(name)\n",
    "    group = group.reset_index()\n",
    "    group[\"e_energy\"] = group.apply(lambda k: temp.apply(lambda x: felt_magnitude(k[\"latitude\"], x[\"latitude\"], k[\"longitude\"], x[\"longitude\"], x[\"energy\"]), axis=1).mean(), axis=1)\n",
    "    del temp\n",
    "    store.put(\"land_prec_earth\", group, format=\"table\", append=True, data_columns=True)\n",
    "    \n",
    "store.close()\n",
    "\n",
    "del earthquake, landslides, l_groups, e_groups, store, group, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The file '../data/data_small_2.h5' is already opened, but in read-only mode.  Please close it before reopening in append mode.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-4fecdaea3527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get Store Info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/data_small_2.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstore2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/data_small.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"landslide\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/etc/anaconda3/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fletcher32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfletcher32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/etc/anaconda3/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m                             hdf_version=tables.get_hdf5_version()))\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/etc/anaconda3/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'can not be written'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/etc/anaconda3/lib/python3.6/site-packages/tables/file.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                     \u001b[0;34m\"The file '%s' is already opened, but \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"in read-only mode.  Please close it before \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     \"reopening in append mode.\" % filename)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;31m# 'w' means that we want to destroy existing contents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The file '../data/data_small_2.h5' is already opened, but in read-only mode.  Please close it before reopening in append mode."
     ]
    }
   ],
   "source": [
    "# Get Store Info\n",
    "\n",
    "store = pd.HDFStore(\"../data/data_small_2.h5\")\n",
    "store2= pd.HDFStore(\"../data/data_small.h5\")\n",
    "df = store.get(\"landslide\")\n",
    "store2.put(\"landslide\", df)\n",
    "del df\n",
    "store2.close()\n",
    "print(store.info())\n",
    "store.close()\n",
    "del store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "landslides = pd.read_hdf(\"../data/data_small_2.h5\", key=\"landslide\")\n",
    "store = pd.HDFStore(\"../data/data_small_3.h5\")\n",
    "store.put(\"landslide\", df)\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
