{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "\n",
    "LANDSLIDE_FILE = \"../data/landslide.h5\"\n",
    "DATA_FILE = \"../data/test/store\"\n",
    "RESULT_FILE = \"../data/test/merged\"\n",
    "\n",
    "\n",
    "# Moving existing landslide data into the same store\n",
    "\n",
    "def landslide_migrate(year):\n",
    "    print(\"Landslide Migration for\", str(year))\n",
    "    landslide_store = pd.read_hdf(LANDSLIDE_FILE, iterator=True, chunksize=1000000)\n",
    "    store = pd.HDFStore(DATA_FILE + \"_\" + str(year) + \".h5\", \"a\")\n",
    "\n",
    "    for chunk in landslide_store:\n",
    "        chunk[\"time\"] = chunk[\"date\"]\n",
    "        del chunk[\"date\"]\n",
    "        chunk = chunk[chunk['time'].dt.year == year]\n",
    "        if not len(chunk):\n",
    "                continue\n",
    "\n",
    "        chunk[\"square\"] = chunk.apply(lambda x: math.sqrt((x[\"latitude\"])**2 + (x[\"longitude\"])**2), axis=1)\n",
    "\n",
    "        chunk.set_index([\"time\"], inplace=True)\n",
    "        if len(chunk):\n",
    "            store.put(\"landslide\", chunk, format=\"table\", append=True, data_columns=True)\n",
    "    \n",
    "    store.close()\n",
    "    landslide_store.close()\n",
    "    del landslide_store, chunk\n",
    "    \n",
    "#reading, processing, and storing the earthquake data \n",
    "\n",
    "def earthquake_migrate(year):\n",
    "    print(\"Earthquake Migration for\", str(year))\n",
    "    # Reading earthquake data into store\n",
    "    def magnitude_from_energy(e):\n",
    "        return (math.log10(e) - 5.24) / 1.44\n",
    "\n",
    "    def lat_lon_dist(lat1, lon1, lat2, lon2):\n",
    "        #https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n",
    "        R = 6363.0\n",
    "\n",
    "        lat1 = math.radians(lat1)\n",
    "        lat2 = math.radians(lat2)\n",
    "        lon1 = math.radians(lon1)\n",
    "        lon2 = math.radians(lon2)\n",
    "        dlon = abs(lon2 - lon1)\n",
    "        dlat = abs(lat2 - lat1)\n",
    "        a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "        return R * c\n",
    "\n",
    "    def felt_magnitude(lat1, lon1, lat2, lon2, energy):\n",
    "        divisor = math.pi * 2 * lat_lon_dist(lat1, lon1, lat2, lon2)\n",
    "        if divisor == 0:\n",
    "            return energy\n",
    "        return energy / divisor\n",
    "\n",
    "    def energy_from_magnitude(m):\n",
    "        #https://www.usgs.gov/natural-hazards/earthquake-hazards/science/earthquake-magnitude-energy-release-and-shaking-intensity?qt-science_center_objects=0#qt-science_center_objects\n",
    "        return math.pow(10,(5.24 + (1.44 * m)))\n",
    "\n",
    "    def gq(path):\n",
    "        df = pd.read_csv(path)[[\"time\", \"latitude\", \"longitude\", \"mag\"]]\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"].map(lambda x: x.year * 100 + x.month), format=\"%Y%m\")\n",
    "        df[\"energy\"] = df[\"mag\"].apply(energy_from_magnitude)\n",
    "        df = df[df['time'].dt.year == year]\n",
    "\n",
    "        df = df.groupby(\"time\")\n",
    "\n",
    "        months = pd.DataFrame()\n",
    "        for name, group in df:\n",
    "            row = {\"latitude\": group[\"latitude\"].mean(), \"longitude\": group[\"longitude\"].mean(), \"energy\": 0, \"time\": name}\n",
    "            row[\"energy\"] = group.apply(lambda x: felt_magnitude(x[\"latitude\"], x[\"longitude\"], row[\"latitude\"], row[\"longitude\"], x[\"energy\"]), axis = 1).sum()\n",
    "            months = months.append(row, ignore_index=True)\n",
    "        return months\n",
    "\n",
    "    df = pd.concat([gq(\"../data/earthquake_northeast.csv\"), gq(\"../data/earthquake_northwest.csv\")])\n",
    "    df[\"square\"] = df.apply(lambda x: math.sqrt((x[\"latitude\"])**2 + (x[\"longitude\"])**2), axis=1)\n",
    "    df.set_index([\"time\"], inplace=True)\n",
    "    \n",
    "    store = pd.HDFStore(DATA_FILE + \"_\" + str(year) + \".h5\", \"a\")\n",
    "    store.put(\"earthquake\", df, format=\"table\", data_columns=True)\n",
    "    store.close()\n",
    "    \n",
    "# Reading, Processing, and Storing the Precipiation Data\n",
    "\n",
    "def precipitation_migrate(year):\n",
    "    print(\"Precipiation Migration for\", str(year))\n",
    "\n",
    "    df = pd.concat([pd.read_csv(\"../data/NorthEast_precipitation_data.csv\"), pd.read_csv(\"../data/NorthWest_precipitation_data.csv\")])\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"Date Time in YYYY-MM-DD\"])\n",
    "    del df[\"Date Time in YYYY-MM-DD\"], df[df.columns[0]]\n",
    "    df.columns = [\"latitude\", \"longitude\", \"precipitation\", \"time\"]\n",
    "    df = df[df['time'].dt.year == year]\n",
    "\n",
    "    df[\"square\"] = df.apply(lambda x: math.sqrt((x[\"latitude\"])**2 + (x[\"longitude\"])**2), axis=1)\n",
    "\n",
    "    df.set_index([\"time\"], inplace=True)\n",
    "\n",
    "    store = pd.HDFStore(DATA_FILE + \"_\" + str(year) + \".h5\", \"a\")\n",
    "    store.put(\"precipitation\", df, format=\"table\", data_columns=True)\n",
    "    store.close()\n",
    "    \n",
    "def merge_data(year):\n",
    "    print(\"Data Migration for\", str(year))\n",
    "\n",
    "    landslide_migrate(year)\n",
    "    earthquake_migrate(year)\n",
    "    precipitation_migrate(year)\n",
    "    # Merging Land and Precipiation\n",
    "\n",
    "    \n",
    "    store = pd.HDFStore(DATA_FILE + \"_\" + str(year) + \".h5\", \"r\")\n",
    "    print(store.info())\n",
    "    store.close()\n",
    "    \n",
    "    \n",
    "    precipitation = pd.read_hdf(DATA_FILE + \"_\" + str(year) + \".h5\", key=\"precipitation\")\n",
    "    earthquake = pd.read_hdf(DATA_FILE + \"_\" + str(year) + \".h5\", key=\"earthquake\")#.groupby(\"time\")\n",
    "    landslide = pd.read_hdf(DATA_FILE + \"_\" + str(year) + \".h5\", key=\"landslide\")#, iterator=True, chunksize=1000000)\n",
    "    precipitation.sort_values(by=\"square\", inplace=True)\n",
    "    earthquake.sort_values(by=\"square\", inplace=True)\n",
    "    precipitation.sort_values(by=\"square\", inplace=True)\n",
    "    landslide.sort_values(by=\"square\", inplace=True)\n",
    "\n",
    "    merged = pd.merge_asof(landslide, precipitation, on=\"square\", by=\"time\")\n",
    "    merged = merged[~merged[\"precipitation\"].isna()]\n",
    "    merged[\"latitude\"] = merged[\"latitude_x\"]\n",
    "    merged[\"longitude\"] = merged[\"longitude_x\"]\n",
    "    del merged[\"latitude_x\"], merged[\"longitude_x\"], merged[\"latitude_y\"], merged[\"longitude_y\"]\n",
    "\n",
    "    merged = pd.merge_asof(merged, earthquake, on=\"square\", by=\"time\")\n",
    "    merged = merged[~merged[\"energy\"].isna()]\n",
    "    merged[\"latitude\"] = merged[\"latitude_x\"]\n",
    "    merged[\"longitude\"] = merged[\"longitude_x\"]\n",
    "    del merged[\"latitude_x\"], merged[\"longitude_x\"], merged[\"latitude_y\"], merged[\"longitude_y\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    dtypes = {\n",
    "        \"Latitude\": \"float32\",\n",
    "        \"Longitude\": \"float32\",\n",
    "        \"Elevation\": \"float32\",\n",
    "        \"Run\": \"float32\",\n",
    "        \"Slope\": \"float32\"\n",
    "    }   \n",
    "    topography = pd.read_csv(\"../data/topography.csv\", dtype=dtypes)\n",
    "    topography.columns = [\"latitude\", \"longitude\", \"elevation\", \"run\", \"slope\"]\n",
    "    topography[\"square\"] = topography.apply(lambda x: math.sqrt((x[\"latitude\"])**2 + (x[\"longitude\"])**2), axis=1)\n",
    "    topography.sort_values(by=\"square\", inplace=True)\n",
    "    merged = pd.merge_asof(merged, topography, on=\"square\")\n",
    "    \n",
    "    \n",
    "    del merged[\"square\"], merged[\"latitude_y\"], merged[\"longitude_y\"]\n",
    "    merged.columns = [\"time\", \"landslide_prob\", \"precipitation\", \"energy\", \"latitude\", \"longitude\", \"elevation\", \"run\", \"slope\"]\n",
    "    \n",
    "    store = pd.HDFStore(RESULT_FILE + \"_\" + str(year) + \".h5\")\n",
    "    store.put(\"merged\", merged, format=\"table\", append=True, data_columns=True)\n",
    "    print(store.info())\n",
    "    store.close()\n",
    "    \n",
    "def create_powerplants(store, landslide):\n",
    "    \n",
    "    def bbox(row, low_lat, low_lon, high_lat, high_lon):\n",
    "        if row[\"latitude\"] > low_lat and row[\"latitude\"] < high_lat and row[\"longitude\"] < high_lon and row[\"longitude\"] > low_lon:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    key = landslide[\"time\"].unique()[0]\n",
    "    landslide = landslide.groupby(\"time\").get_group(key)\n",
    "    landslide.reset_index(inplace=True)\n",
    "    landslide[\"square\"] = landslide.apply(lambda x: math.sqrt((x[\"latitude\"] - 39.721514)**2 + (x[\"longitude\"] - -80.519571)**2), axis=1)\n",
    "\n",
    "    dtypes = {\n",
    "        \"LATITUDE\": \"float32\",\n",
    "        \"LONGITUDE\": \"float32\",\n",
    "        \"NAME\": \"object\",\n",
    "        \"CITY\": \"object\",\n",
    "        \"STATE\": \"object\"\n",
    "    }   \n",
    "    powerplants = pd.read_csv(\"../data/powerplants.csv\", usecols=[\"NAME\", \"CITY\", \"STATE\", \"LATITUDE\", \"LONGITUDE\"], dtype=dtypes)\n",
    "    powerplants.columns = [\"name\", \"city\", \"state\", \"latitude\", \"longitude\"]\n",
    "    powerplants = powerplants[powerplants.apply(lambda x: bbox(x, 39.721514, -80.519571, 45.313525, -71.087031), axis=1)]\n",
    "    powerplants[\"square\"] = powerplants.apply(lambda x: math.sqrt((x[\"latitude\"] - 39.721514)**2 + (x[\"longitude\"] - -80.519571)**2), axis=1)\n",
    "    \n",
    "    landslide.sort_values(by=\"square\", inplace=True)\n",
    "    powerplants.sort_values(by=\"square\", inplace=True)\n",
    "    merged = pd.merge_asof(powerplants, landslide, on=\"square\", direction=\"nearest\")\n",
    "    merged = merged[[\"name\", \"city\", \"state\", \"latitude_x\", \"longitude_x\", \"precipitation\", \"energy\", \"elevation\", \"run\", \"slope\"]]\n",
    "    merged.columns = [\"name\", \"city\", \"state\", \"latitude\", \"longitude\", \"precipitation\", \"energy\", \"elevation\", \"run\", \"slope\"]\n",
    "    store.put(\"powerplants\", merged, format=\"table\", append=True, data_columns=True)\n",
    "    \n",
    "def combine_files(years):\n",
    "    print(\"Combining Files\")\n",
    "    store = pd.HDFStore(RESULT_FILE + \".h5\", \"a\")\n",
    "    for year in years:\n",
    "        landslide = pd.read_hdf(RESULT_FILE + \"_\" + str(year) + \".h5\", key=\"merged\")\n",
    "        store.put(\"merged\", landslide, format=\"table\", append=True, data_columns=True)\n",
    "        if year == years[-1]:\n",
    "            create_powerplants(store, landslide)\n",
    "    print(store.info())\n",
    "    store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Migration for 2015\n",
      "Landslide Migration for 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/etc/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earthquake Migration for 2015\n",
      "Precipiation Migration for 2015\n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: ../data/test/store_2015.h5\n",
      "/earthquake               frame_table  (typ->appendable,nrows->24,ncols->4,indexers->[index],dc->[energy,latitude,longitude,square])              \n",
      "/landslide                frame_table  (typ->appendable,nrows->17764464,ncols->4,indexers->[index],dc->[latitude,longitude,landslide_prob,square])\n",
      "/precipitation            frame_table  (typ->appendable,nrows->123468,ncols->4,indexers->[index],dc->[latitude,longitude,precipitation,square])   \n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: ../data/test/merged_2015.h5\n",
      "/merged            frame_table  (typ->appendable,nrows->14643216,ncols->9,indexers->[index],dc->[time,landslide_prob,precipitation,energy,latitude,longitude,elevation,run,slope])\n",
      "Data Migration for 2016\n",
      "Landslide Migration for 2016\n"
     ]
    }
   ],
   "source": [
    "years = [2015, 2016, 2017, 2018, 2019]\n",
    "for year in years:\n",
    "    merge_data(year)\n",
    "combine_files(years)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
